{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA3p0Xqk81mj"
      },
      "source": [
        "# Question 1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJWjm0542JAI",
        "outputId": "00477bf3-3e45-4a69-a43c-8e3662fb3046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "455jLOVq3xu5"
      },
      "outputs": [],
      "source": [
        "w2v.save_word2vec_format('word2vec-google-news-300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbGceDiptls3"
      },
      "source": [
        "Question 1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1n2mAqR8zIW",
        "outputId": "2469bee4-8385-49a9-f074-13f6ca3dbbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar word to 'student' is 'students' with  cosine similarity: 0.7294867038726807\n",
            "Similar word to 'Apple' is 'Apple_AAPL' with  cosine similarity: 0.7456986308097839\n",
            "Similar word to 'apple' is 'apples' with  cosine similarity: 0.720359742641449\n"
          ]
        }
      ],
      "source": [
        "similar_to_student = w2v.most_similar('student', topn=1)[0]\n",
        "print(f\"Similar word to 'student' is '{similar_to_student[0]}' with  cosine similarity: {similar_to_student[1]}\")\n",
        "\n",
        "similar_to_Apple = w2v.most_similar('Apple', topn=1)[0]\n",
        "print(f\"Similar word to 'Apple' is '{similar_to_Apple[0]}' with  cosine similarity: {similar_to_Apple[1]}\")\n",
        "\n",
        "similar_to_apple = w2v.most_similar('apple', topn=1)[0]\n",
        "print(f\"Similar word to 'apple' is '{similar_to_apple[0]}' with  cosine similarity: {similar_to_apple[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHvUpSo6M4k0"
      },
      "source": [
        "Question 1.2 a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuS5_JT88gw2",
        "outputId": "2cf5ed7a-c14f-477e-9a2a-9349992be6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: 14989 sentences, Labels: {'I-ORG', 'I-LOC', 'B-MISC', 'B-LOC', 'I-PER', 'I-MISC', 'B-ORG', 'O'}\n",
            "Development set: 3468 sentences, Labels: {'O', 'I-LOC', 'B-MISC', 'I-PER', 'I-MISC', 'I-ORG'}\n",
            "Test set: 3683 sentences, Labels: {'O', 'I-LOC', 'B-MISC', 'B-LOC', 'I-PER', 'I-MISC', 'B-ORG', 'I-ORG'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "train_data = pd.read_csv(\"eng.train\", sep=' ', header=None, names=['word', 'pos', 'chunk', 'label'], skip_blank_lines=False)\n",
        "dev_data = pd.read_csv(\"eng.testa\", sep=' ', header=None, names=['word', 'pos', 'chunk', 'label'], skip_blank_lines=False)\n",
        "test_data = pd.read_csv(\"eng.testb\", sep=' ', header=None, names=['word', 'pos', 'chunk', 'label'], skip_blank_lines=False)\n",
        "\n",
        "# Function to get number of sentences and unique labels\n",
        "def get_info(data):\n",
        "    num_sentences = data['word'].isnull().sum()\n",
        "    labels = data['label'].dropna().tolist()\n",
        "    unique_labels = set(labels)\n",
        "    return num_sentences, unique_labels\n",
        "\n",
        "# Get information\n",
        "train_info = get_info(train_data)\n",
        "dev_info = get_info(dev_data)\n",
        "test_info = get_info(test_data)\n",
        "\n",
        "print(f'Training set: {train_info[0]} sentences, Labels: {train_info[1]}')\n",
        "print(f'Development set: {dev_info[0]} sentences, Labels: {dev_info[1]}')\n",
        "print(f'Test set: {test_info[0]} sentences, Labels: {test_info[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tBE3jwEZsh"
      },
      "source": [
        "1.2b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqz_SyshEYkj",
        "outputId": "1751e879-9552-419c-fbce-a3d39c874ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              word  label\n",
            "47         Germany  I-LOC\n",
            "48              's      O\n",
            "49  representative      O\n",
            "50              to      O\n",
            "51             the      O\n",
            "52        European  I-ORG\n",
            "53           Union  I-ORG\n",
            "54              's      O\n",
            "55      veterinary      O\n",
            "56       committee      O\n",
            "57          Werner  I-PER\n",
            "58       Zwingmann  I-PER\n",
            "59            said      O\n",
            "60              on      O\n",
            "61       Wednesday      O\n",
            "62       consumers      O\n",
            "63          should      O\n",
            "64             buy      O\n",
            "65       sheepmeat      O\n",
            "66            from      O\n",
            "67       countries      O\n",
            "68           other      O\n",
            "69            than      O\n",
            "70         Britain  I-LOC\n",
            "71           until      O\n",
            "72             the      O\n",
            "73      scientific      O\n",
            "74          advice      O\n",
            "75             was      O\n",
            "76         clearer      O\n",
            "77               .      O\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def find_example_sentence(data):\n",
        "    current_sentence = []\n",
        "    current_entity = \"\"\n",
        "    current_label = None\n",
        "    multi_word_entities = Counter()\n",
        "    for index, row in data.iterrows():\n",
        "        if pd.isnull(row['word']):\n",
        "            # Sentence boundary\n",
        "            if len(multi_word_entities) >= 2:\n",
        "                return current_sentence\n",
        "            # Reset for next sentence\n",
        "            current_sentence = []\n",
        "            multi_word_entities = Counter()\n",
        "        else:\n",
        "            current_sentence.append(row)\n",
        "            prefix, label = row['label'].split('-') if '-' in row['label'] else (None, None)\n",
        "            if prefix == 'B' or (prefix == 'I' and label != current_label):\n",
        "                # New entity\n",
        "                if current_entity.count(' ') >= 1:  # Check if the entity has more than one word\n",
        "                    multi_word_entities[current_label] += 1\n",
        "                current_entity = row['word']\n",
        "                current_label = label\n",
        "            elif prefix == 'I':\n",
        "                # Continuation of the current entity\n",
        "                current_entity += ' ' + row['word']\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('eng.train', sep=' ', header=None, names=['word', 'pos', 'chunk', 'label'], skip_blank_lines=False)\n",
        "\n",
        "# Find and print example sentence\n",
        "example_sentence = find_example_sentence(train_data)\n",
        "example_sentence_df = pd.DataFrame(example_sentence)\n",
        "print(example_sentence_df[['word', 'label']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwbhaNBJEeHb",
        "outputId": "feaf084e-575d-46d2-f944-2300b0e9545d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Germany (LOC)\n",
            "European Union (ORG)\n",
            "Werner Zwingmann (PER)\n",
            "Britain (LOC)\n"
          ]
        }
      ],
      "source": [
        "def list_named_entities(sentence):\n",
        "    named_entities = []\n",
        "    current_entity = []\n",
        "    current_label = None\n",
        "    for _, row in sentence.iterrows():\n",
        "        label_parts = row['label'].split('-')\n",
        "        prefix, label = label_parts if len(label_parts) > 1 else (None, None)\n",
        "        if prefix == 'B' or (prefix == 'I' and label != current_label):\n",
        "            # New named entity\n",
        "            if current_entity:\n",
        "                named_entities.append((' '.join(current_entity), current_label))\n",
        "            current_entity = [row['word']]\n",
        "            current_label = label\n",
        "        elif prefix == 'I':\n",
        "            # Continuation of current named entity\n",
        "            current_entity.append(row['word'])\n",
        "    # Don't forget the last named entity\n",
        "    if current_entity:\n",
        "        named_entities.append((' '.join(current_entity), current_label))\n",
        "    return named_entities\n",
        "\n",
        "named_entities = list_named_entities(example_sentence_df)\n",
        "for entity, label in named_entities:\n",
        "    print(f'{entity} ({label})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelwHUYSEMab"
      },
      "source": [
        "1.3a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzDcWfGFXslG",
        "outputId": "c59c8b28-534b-436d-f02f-70ccca632391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvcFNAIxOYrn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from seqeval.metrics import f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClowCORLOd8O"
      },
      "outputs": [],
      "source": [
        "def handle_oov_words(word, surrounding_words, embeddings):\n",
        "    global zero_vector_count  # for counter of zero vector\n",
        "    global average_embedding_count # for embeeding use count\n",
        "\n",
        "    if word in embeddings:\n",
        "        return embeddings[word]\n",
        "\n",
        "    elif surrounding_words:\n",
        "        surrounding_embeddings = [embeddings[w] for w in surrounding_words if w in embeddings]\n",
        "        if surrounding_embeddings:\n",
        "            # computing average\n",
        "            avg_embedding = np.mean(surrounding_embeddings, axis=0)\n",
        "            average_embedding_count += 1\n",
        "            return avg_embedding\n",
        "    # only return zero vector if otherwise\n",
        "    zero_vector_count += 1\n",
        "    return np.zeros(embeddings.vector_size)\n",
        "\n",
        "def get_embedding_for_sentence(sentence, embeddings):\n",
        "    sentence_embeddings = []\n",
        "    for i, word in enumerate(sentence):\n",
        "        surrounding_words = sentence[max(0, i-2):i] + sentence[i+1:min(len(sentence), i+3)]\n",
        "        word_embedding = handle_oov_words(word, surrounding_words, embeddings)\n",
        "        sentence_embeddings.append(word_embedding)\n",
        "    return sentence_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzgY4H6BOf5_",
        "outputId": "331210e2-d552-4a78-caf8-1ca484291aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of zero vectors used: 4339\n",
            "Number of average embeddings used: 52176\n"
          ]
        }
      ],
      "source": [
        "sentences_list = []\n",
        "labels_list = []\n",
        "current_sentence = []\n",
        "current_labels = []\n",
        "\n",
        "zero_vector_count = 0\n",
        "average_embedding_count = 0\n",
        "\n",
        "with open('eng.train', 'r') as file:\n",
        "    for line in file:\n",
        "        # Split lines into words and labels\n",
        "        tokens = line.strip().split()\n",
        "        if tokens:  # If line is not blank\n",
        "            word, pos, chunk, label = tokens\n",
        "            current_sentence.append(word)\n",
        "            current_labels.append(label)\n",
        "        else:  # If line is blank (end of sentence)\n",
        "            if current_sentence and current_labels:  # If there's a sentence/labels to add\n",
        "                sentences_list.append(current_sentence)\n",
        "                labels_list.append(current_labels)\n",
        "                # Reset current_sentence and current_labels for the next sentence\n",
        "                current_sentence = []\n",
        "                current_labels = []\n",
        "\n",
        "    # code for opening file gotten from chatgpt\n",
        "\n",
        "if current_sentence and current_labels:\n",
        "    sentences_list.append(current_sentence)\n",
        "    labels_list.append(current_labels)\n",
        "\n",
        "sentence_embeddings = [get_embedding_for_sentence(sentence, w2v) for sentence in sentences_list]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = [label_encoder.fit_transform(labels) for labels in labels_list]\n",
        "\n",
        "print(f'Number of zero vectors used: {zero_vector_count}')\n",
        "print(f'Number of average embeddings used: {average_embedding_count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0567AXpK5vh",
        "outputId": "80c3f42a-d4c9-49ce-d123-5ac948c25048"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d18cd1630131>:64: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  embedding_matrix[index] = torch.from_numpy(w2v[word])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.1524\n",
            "Time taken for epoch 1: 237.11 seconds\n",
            "F1 Score: 0.8497\n",
            "Epoch 2/10, Loss: 0.0690\n",
            "Time taken for epoch 2: 227.16 seconds\n",
            "F1 Score: 0.8832\n",
            "Epoch 3/10, Loss: 0.0375\n",
            "Time taken for epoch 3: 236.30 seconds\n",
            "F1 Score: 0.8959\n",
            "Epoch 4/10, Loss: 0.0230\n",
            "Time taken for epoch 4: 238.22 seconds\n",
            "F1 Score: 0.8967\n",
            "Epoch 5/10, Loss: 0.0176\n",
            "Time taken for epoch 5: 237.60 seconds\n",
            "F1 Score: 0.8958\n",
            "Early stopping...\n",
            "Total time taken for all epochs: 1227.53 seconds\n"
          ]
        }
      ],
      "source": [
        "def load_conll_data(filename):\n",
        "    data = pd.read_csv(filename, sep=' ', header=None, names=['word', 'pos', 'chunk', 'label'], skip_blank_lines=False)\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for index, row in data.iterrows():\n",
        "        if pd.isnull(row['word']):\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            word = row['word']\n",
        "            label = row['label']\n",
        "            if word in w2v:\n",
        "                sentence.append((word, label))\n",
        "    return sentences\n",
        "\n",
        "train_sentences = load_conll_data('eng.train')\n",
        "dev_sentences = load_conll_data('eng.testa')\n",
        "test_sentences = load_conll_data('eng.testb')\n",
        "\n",
        "# Create vocabulary and label sets\n",
        "word_vocab = set(word for sentence in train_sentences + dev_sentences + test_sentences for word, _ in sentence)\n",
        "label_set = set(label for sentence in train_sentences + dev_sentences + test_sentences for _, label in sentence)\n",
        "\n",
        "# Map words and labels to indices\n",
        "word_to_index = {word: i for i, word in enumerate(word_vocab)}\n",
        "label_to_index = {label: i for i, label in enumerate(label_set)}\n",
        "index_to_label = {i: label for label, i in label_to_index.items()}\n",
        "\n",
        "# Define a function to convert a sentence to input and target tensors\n",
        "def sentence_to_tensors(sentence):\n",
        "    words, labels = zip(*sentence)\n",
        "    word_indices = torch.tensor([word_to_index[word] for word in words], dtype=torch.long)\n",
        "    label_indices = torch.tensor([label_to_index[label] for label in labels], dtype=torch.long)\n",
        "    return word_indices, label_indices\n",
        "\n",
        "# Convert sentences to tensors\n",
        "train_data = [sentence_to_tensors(sentence) for sentence in train_sentences]\n",
        "dev_data = [sentence_to_tensors(sentence) for sentence in dev_sentences]\n",
        "test_data = [sentence_to_tensors(sentence) for sentence in test_sentences]\n",
        "\n",
        "# Define the NER model\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, pretrained_embeddings):\n",
        "        super(NERModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "        self.embedding.weight.requires_grad = False  # Freeze the embeddings\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, label_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# Create word embeddings matrix\n",
        "embedding_dim = 300\n",
        "vocab_size = len(word_vocab)\n",
        "embedding_matrix = torch.zeros(vocab_size, embedding_dim)\n",
        "for word, index in word_to_index.items():\n",
        "    if word in w2v:\n",
        "        embedding_matrix[index] = torch.from_numpy(w2v[word])\n",
        "\n",
        "# Instantiate the NER model\n",
        "hidden_dim = 256\n",
        "label_size = len(label_set)\n",
        "ner_model = NERModel(embedding_dim, hidden_dim, vocab_size, label_size, embedding_matrix)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ner_model.parameters())\n",
        "\n",
        "# Define a function to train the NER model\n",
        "def train_ner_model(model, train_data, dev_data, optimizer, criterion, num_epochs=10):\n",
        "    total_start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for word_indices, label_indices in train_data:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(word_indices.unsqueeze(0))  # Add a batch dimension\n",
        "            labels = label_indices.unsqueeze(0)  # Add a batch dimension\n",
        "            loss = criterion(outputs.view(-1, label_size), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_data)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_time = epoch_end_time - epoch_start_time\n",
        "        print(f'Time taken for epoch {epoch+1}: {epoch_time:.2f} seconds')\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        with torch.no_grad():\n",
        "            for word_indices, label_indices in dev_data:\n",
        "                outputs = model(word_indices.unsqueeze(0))\n",
        "                _, predicted = torch.max(outputs, 2)\n",
        "                predicted = predicted.squeeze(0).tolist()\n",
        "                true_labels.append([index_to_label[i] for i in label_indices.tolist()])  # Convert indices to labels\n",
        "                predictions.append([index_to_label[i] for i in predicted])\n",
        "        f1 = f1_score(true_labels, predictions)\n",
        "        print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if epoch > 0 and f1 <= max_f1:\n",
        "            print('Early stopping...')\n",
        "            break\n",
        "        max_f1 = f1\n",
        "    total_end_time = time.time()\n",
        "    total_time = total_end_time - total_start_time\n",
        "    print(f'Total time taken for all epochs: {total_time:.2f} seconds')\n",
        "\n",
        "# Train the NER model\n",
        "train_ner_model(ner_model, train_data, dev_data, optimizer, criterion, num_epochs=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnTTYJnkuUjA",
        "outputId": "d9c94cfa-76fa-4ba6-a80c-bad516d8e9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Sentence:\n",
            "EU rejects German call boycott British lamb\n",
            "Predicted Labels:\n",
            "['I-ORG', 'O', 'I-MISC', 'O', 'O', 'I-MISC', 'O']\n"
          ]
        }
      ],
      "source": [
        "# Define a function to predict labels for a sentence\n",
        "def predict_labels(model, sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        word_indices, _ = sentence_to_tensors(sentence)\n",
        "        outputs = model(word_indices.unsqueeze(0))\n",
        "        _, predicted = torch.max(outputs, 2)\n",
        "        predicted = predicted.squeeze(0).tolist()\n",
        "        labels = [index_to_label[i] for i in predicted]\n",
        "        return labels\n",
        "\n",
        "# Test the NER model on a sentence\n",
        "example_sentence = train_sentences[0]\n",
        "predicted_labels = predict_labels(ner_model, example_sentence)\n",
        "print('Example Sentence:')\n",
        "print(' '.join([word for word, _ in example_sentence]))\n",
        "print('Predicted Labels:')\n",
        "print(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqvnwWmFTZaU",
        "outputId": "fbcd39d0-2476-4420-af89-d498cb46b291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.85\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "# Prepare predicted and ground truth labels\n",
        "predicted_labels = [predict_labels(ner_model, sentence) for sentence in test_sentences]\n",
        "true_labels = [[label for _, label in sentence] for sentence in test_sentences]\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "print(f'F1 Score: {f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJae9BWrXUSq"
      },
      "outputs": [],
      "source": [
        "# #Q2\n",
        "\n",
        "# from random import randint\n",
        "# train = pd.read_csv('train.csv', usecols=[\"label-coarse\", \"text\"])\n",
        "# test = pd.read_csv('test.csv', usecols=[\"label-coarse\", \"text\"])\n",
        "# merge1 = randint(0,5)\n",
        "# merge2 = randint(0,5)\n",
        "# while merge1==merge2:\n",
        "#     merge2 = randint(0,5)\n",
        "# train = train.replace([merge1,merge2],\"OTHERS\")\n",
        "# test = test.replace([merge1,merge2],\"OTHERS\")\n",
        "# dev = train.sample(n=500)\n",
        "# train = train.drop(dev.index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Fn5vCR2oFS"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08jYiz-vM1KZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c10856-b3b9-4394-e5f4-d4b97dab0739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected classes: ['0', '1', '5', '2']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Randomly select 4 unique classes from the 6 different coarse labels\n",
        "selected_classes = np.random.choice(train_df['label-coarse'].unique(), 4, replace=False)\n",
        "# Ensure selected_classes is a list of strings\n",
        "selected_classes = [str(cls) for cls in selected_classes]\n",
        "print(f\"Selected classes: {selected_classes}\")\n",
        "\n",
        "# Make sure the 'label-coarse' column is of string type before comparison\n",
        "train_df['label-coarse'] = train_df['label-coarse'].astype(str)\n",
        "test_df['label-coarse'] = test_df['label-coarse'].astype(str)\n",
        "\n",
        "# Apply the lambda function to update the labels\n",
        "train_df['label-coarse'] = train_df['label-coarse'].apply(lambda x: x if x in selected_classes else 'OTHERS')\n",
        "test_df['label-coarse'] = test_df['label-coarse'].apply(lambda x: x if x in selected_classes else 'OTHERS')\n",
        "\n",
        "# Now, the label_encoder should work without type error\n",
        "label_encoder = LabelEncoder()\n",
        "labels_train = label_encoder.fit_transform(train_df['label-coarse'].values)\n",
        "labels_test = label_encoder.transform(test_df['label-coarse'].values)\n",
        "\n",
        "# Preprocessing function for text\n",
        "def preprocess_text(sentence):\n",
        "    words = sentence.lower().split()  # simple tokenization and lowercasing\n",
        "    # Remove stop words, punctuation, etc. if necessary\n",
        "    return words\n",
        "\n",
        "# Function to convert sentences to a matrix of word embeddings\n",
        "def sentence_to_avg(sentence, w2v_model):\n",
        "    words = preprocess_text(sentence)\n",
        "    embedding_matrix = [w2v_model[word] for word in words if word in w2v_model]\n",
        "    if not embedding_matrix:  # Handle sentences without any words in vocabulary\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    avg_embedding = np.mean(embedding_matrix, axis=0)\n",
        "    return avg_embedding\n",
        "\n",
        "# Preprocess and encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_train = label_encoder.fit_transform(train_df['label-coarse'].values)\n",
        "labels_test = label_encoder.transform(test_df['label-coarse'].values)\n",
        "\n",
        "# Convert sentences to average embeddings\n",
        "X_train = np.array([sentence_to_avg(sentence, w2v) for sentence in train_df['text'].values])\n",
        "X_test = np.array([sentence_to_avg(sentence, w2v) for sentence in test_df['text'].values])\n",
        "\n",
        "# Split the training data into training and development sets\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_train, labels_train, test_size=500/len(train_df), random_state=42)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_dev_tensor = torch.tensor(X_dev, dtype=torch.float)\n",
        "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(labels_test, dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert sentences to a matrix of word embeddings\n",
        "def sentence_to_sum(sentence, w2v_model):\n",
        "    words = preprocess_text(sentence)\n",
        "    embedding_matrix = [w2v_model[word] for word in words if word in w2v_model]\n",
        "    if not embedding_matrix:  # Handle sentences without any words in vocabulary\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    avg_embedding = np.sum(embedding_matrix, axis=0)\n",
        "    return avg_embedding"
      ],
      "metadata": {
        "id": "MVgYJd9UC66v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert sentences to a matrix of word embeddings\n",
        "def sentence_to_max(sentence, w2v_model):\n",
        "    words = preprocess_text(sentence)\n",
        "    embedding_matrix = [w2v_model[word] for word in words if word in w2v_model]\n",
        "    if not embedding_matrix:  # Handle sentences without any words in vocabulary\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    avg_embedding = np.max(embedding_matrix, axis=0)\n",
        "    return avg_embedding"
      ],
      "metadata": {
        "id": "14_cFtxMC7u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvdmkCeJ2uIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbf78d4-795e-40cb-90fe-2a3ca761a1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Dev Accuracy: 0.6240\n",
            "Epoch 2, Dev Accuracy: 0.6960\n",
            "Epoch 3, Dev Accuracy: 0.7220\n",
            "Epoch 4, Dev Accuracy: 0.7380\n",
            "Epoch 5, Dev Accuracy: 0.7320\n",
            "Epoch 6, Dev Accuracy: 0.7600\n",
            "Epoch 7, Dev Accuracy: 0.7500\n",
            "Epoch 8, Dev Accuracy: 0.7640\n",
            "Epoch 9, Dev Accuracy: 0.7700\n",
            "Epoch 10, Dev Accuracy: 0.7740\n",
            "Epoch 11, Dev Accuracy: 0.7840\n",
            "Epoch 12, Dev Accuracy: 0.7940\n",
            "Epoch 13, Dev Accuracy: 0.7960\n",
            "Epoch 14, Dev Accuracy: 0.8040\n",
            "Epoch 15, Dev Accuracy: 0.7720\n",
            "Epoch 16, Dev Accuracy: 0.8000\n",
            "Epoch 17, Dev Accuracy: 0.8140\n",
            "Epoch 18, Dev Accuracy: 0.8160\n",
            "Epoch 19, Dev Accuracy: 0.8100\n",
            "Epoch 20, Dev Accuracy: 0.8120\n",
            "Test Accuracy: 0.8620\n",
            "Time taken: 16.036020278930664\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the model with dropout for regularization\n",
        "class QuestionClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, dropout_rate=0.5):\n",
        "        super(QuestionClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()  # make sure x is float\n",
        "        out = self.dropout(self.relu(self.fc1(x)))\n",
        "        out = self.dropout(self.relu(self.fc2(out)))\n",
        "        out = self.fc3(out)\n",
        "        return self.softmax(out)\n",
        "\n",
        "# Assuming `embedding_matrix` is of shape (vocab_size, embedding_dim)\n",
        "# and `vocab_size` is the total number of words and `embedding_dim` is the dimension of the word vectors.\n",
        "embedding_dim = w2v.vector_size  # This should be the size of your word vectors\n",
        "# output_size = len(np.unique(y_train))  # This should be the number of unique classes\n",
        "output_size = 5  # This should now be 5 since there are 5 classes including 'OTHERS'\n",
        "hidden_size = 128  # You can tune this\n",
        "dropout_rate = 0.5  # Regularization with dropout\n",
        "\n",
        "# Initialize the model\n",
        "model = QuestionClassifier(input_size=embedding_dim, output_size=output_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "# Define the loss function and the optimizer with L2 regularization (weight decay)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Added weight decay for L2 regularization\n",
        "\n",
        "# Add early stopping logic based on development set accuracy\n",
        "best_dev_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 3  # number of epochs with no improvement after which training will be stopped\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # You may want to use early stopping based on dev set performance\n",
        "total_start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on development set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accuracy = correct / total\n",
        "        print(f'Epoch {epoch + 1}, Dev Accuracy: {accuracy:.4f}')\n",
        "\n",
        "        # Early stopping logic\n",
        "    if accuracy > best_dev_acc:\n",
        "        best_dev_acc = accuracy\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == n_epochs_stop and epoch!=num_epochs-1:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "total_end_time = time.time()\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(\"Time taken:\",total_end_time-total_start_time)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}